{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toturial 1: Fine-tuning CellFM for Cell Type Annotation\n",
    "In this tutorial, we will demonstrate how to fine-tune a pre-trained CellFM model to perform cell type annotation on a new single-cell dataset. The workflow consists of the following stages:\n",
    "\n",
    "1. Data Preprocessing\n",
    "\n",
    "2. Data Loading\n",
    "\n",
    "3. Model construct\n",
    "\n",
    "4. Weight Loading\n",
    "\n",
    "5. Fine-tuning\n",
    "\n",
    "6. Result Visualization\n",
    "\n",
    "Before starting, import the following packages and set up the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/yinghsin/M2LAB/CellFM-torch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir(\"..\")\n",
    "os.chdir(\"/home/yinghsin/M2LAB/CellFM-torch\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from layers.utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from model import Cell_FM\n",
    "\n",
    "cfg = Config_80M()\n",
    "cfg.ecs_threshold = 0.8\n",
    "cfg.ecs = True\n",
    "cfg.add_zero = True\n",
    "cfg.pad_zero = True\n",
    "cfg.use_bs = 8 #this is batch_size batch size for training\n",
    "cfg.mask_ratio = 0.5\n",
    "### Main param ###\n",
    "cfg.dataset = \"Pancrm4\"\n",
    "cfg.feature_col = \"cell_type\"\n",
    "cfg.ckpt_path = \"/bigdat2/user/shanggny/checkpoint/para80m/6300w_18000_19479-1_38071.ckpt\"\n",
    "cfg.device = \"cuda:0\"\n",
    "cfg.epoch = 5\n",
    "#cfg.num_cls = 1 # by default 之後會改成cfg.num_cls = int(batch[\"feat\"].max().item() + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda device count:\", torch.cuda.device_count())\n",
    "cfg.device = \"cuda:0\"\n",
    "#cfg.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import pickle as pk\n",
    "import anndata as ad\n",
    "import requests as rq\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm,trange\n",
    "from functools import partial\n",
    "from scipy.sparse import csr_matrix as csr\n",
    "from scipy.sparse import csc_matrix as csc\n",
    "from multiprocessing import Process,Pool\n",
    "from sklearn.neighbors import NearestNeighbors as NN\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The data preprocessing workflow is identical to the original CellFM implementation. Please follow the same steps as described in the original [CellFM documentation](https://github.com/biomed-AI/CellFM/blob/main/tutorials/process.ipynb) to prepare your datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Load an .h5ad single-cell dataset and return a PyTorch DataLoader ready for model training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin shape: (32472, 15148)\n",
      "origin shape: (32472, 15148)\n"
     ]
    }
   ],
   "source": [
    "adata_path = \"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\"\n",
    "def load_data(adata_path, mode=\"train\"):\n",
    "    adata = read_h5ad(adata_path)\n",
    "    adata.obs['celltype'] = adata.obs['cell_type']\n",
    "    adata.obs['feat'] = adata.obs[cfg.feature_col].cat.codes.values\n",
    "    cfg.num_cls = len(adata.obs['feat'].unique())\n",
    "    \n",
    "    adata.obs['batch_id'] = 0\n",
    "    if mode == \"train\":\n",
    "        adata.obs['train'] = 0\n",
    "        dataset = SCrna(adata, mode=\"train\")\n",
    "        prep = Prepare(cfg.nonz_len, pad=0, mask_ratio=cfg.mask_ratio)\n",
    "        loader = build_dataset(\n",
    "            dataset,\n",
    "            prep=prep,\n",
    "            batch_size=cfg.use_bs,\n",
    "            pad_zero=cfg.pad_zero,\n",
    "            drop=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "    if mode== \"test\":\n",
    "        adata.obs['train'] = 2\n",
    "        dataset = SCrna(adata, mode=\"test\")\n",
    "        prep = Prepare(cfg.nonz_len, pad=0, mask_ratio=cfg.mask_ratio)\n",
    "        loader = build_dataset(\n",
    "            dataset,\n",
    "            prep=prep,\n",
    "            batch_size=cfg.use_bs,\n",
    "            drop=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "    return loader\n",
    "\n",
    "train_loader = load_data(\n",
    "    \"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\",\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "test_loader = load_data(\n",
    "    \"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\",\n",
    "    mode=\"test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd = /home/yinghsin/M2LAB/CellFM-torch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yinghsin/M2LAB/CellFM-torch\")\n",
    "print(\"cwd =\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construct\n",
    "To make CellFM more flexible and reusable across different tasks, we retain its core masked recovery module and re-implement it in PyTorch as a class called `Cell_FM`. This class can either:\n",
    "\n",
    "- Load a set of pre-trained weights and the corresponding configuration, or\n",
    "\n",
    "- Be trained from scratch.\n",
    "\n",
    "`Cell_FM` allows fine-tuning on new datasets to perform masked expression recovery and outputs a cls_token representing each cell.\n",
    "\n",
    "For tasks that require cell type classification, we can simply add a linear classification layer on top of the cls_token. In this tutorial, we use a single hidden-layer linear model for this purpose.\n",
    "\n",
    "Below, we demonstrate how to leverage the `Cell_FM` module to build a custom PyTorch model suitable for task-specific fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finetune_Cell_FM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Finetune_Cell_FM, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.num_cls = cfg.num_cls\n",
    "        self.extractor = Cell_FM(27855, self.cfg, ckpt_path=self.cfg.ckpt_path, device=self.cfg.device) # n_gene, cfg=config_80M()\n",
    "        # notion: 27855 is the orignal pre_train gene set of 80M CellFM model\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(self.cfg.enc_dims, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, self.num_cls)\n",
    "        )\n",
    "    \n",
    "    def forward(self, raw_nzdata,\n",
    "                dw_nzdata,\n",
    "                ST_feat,\n",
    "                nonz_gene,\n",
    "                mask_gene,\n",
    "                zero_idx):\n",
    "        \n",
    "        mask_loss, cls_token = self.extractor(\n",
    "                raw_nzdata,\n",
    "                dw_nzdata,\n",
    "                ST_feat,\n",
    "                nonz_gene,\n",
    "                mask_gene,\n",
    "                zero_idx\n",
    "            )\n",
    "        \n",
    "        cls = self.cls(cls_token)\n",
    "\n",
    "        return cls, mask_loss, cls_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Loading\n",
    "\n",
    "You can easily load pre-trained weights using the `load_model` function in the `Cell_FM` module. Setting the option to False will initialize the model from scratch.\n",
    "\n",
    "Note: During fine-tuning, we only unfreeze the `cls.` and `encoder` layers. This strategy reduces the computational cost and memory usage while still allowing effective adaptation to the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "extractor.net.encoder.0.attn.q_proj.weight\n",
      "extractor.net.encoder.0.attn.k_proj.weight\n",
      "extractor.net.encoder.0.attn.v_proj.weight\n",
      "extractor.net.encoder.0.attn.u_proj.weight\n",
      "extractor.net.encoder.0.attn.o_proj.weight\n",
      "extractor.net.encoder.0.ffn.u_proj.weight\n",
      "extractor.net.encoder.0.ffn.v_proj.weight\n",
      "extractor.net.encoder.0.ffn.o_proj.weight\n",
      "extractor.net.encoder.0.post_norm1.weight\n",
      "extractor.net.encoder.0.post_norm1.bias\n",
      "extractor.net.encoder.0.post_norm2.weight\n",
      "extractor.net.encoder.0.post_norm2.bias\n",
      "extractor.net.encoder.1.attn.q_proj.weight\n",
      "extractor.net.encoder.1.attn.k_proj.weight\n",
      "extractor.net.encoder.1.attn.v_proj.weight\n",
      "extractor.net.encoder.1.attn.u_proj.weight\n",
      "extractor.net.encoder.1.attn.o_proj.weight\n",
      "extractor.net.encoder.1.ffn.u_proj.weight\n",
      "extractor.net.encoder.1.ffn.v_proj.weight\n",
      "extractor.net.encoder.1.ffn.o_proj.weight\n",
      "extractor.net.encoder.1.post_norm1.weight\n",
      "extractor.net.encoder.1.post_norm1.bias\n",
      "extractor.net.encoder.1.post_norm2.weight\n",
      "extractor.net.encoder.1.post_norm2.bias\n",
      "cls.0.weight\n",
      "cls.0.bias\n",
      "cls.1.weight\n",
      "cls.1.bias\n",
      "cls.3.weight\n",
      "cls.3.bias\n",
      "[Load PyTorch Weight]\n",
      "Missing keys: []\n",
      "Unexpected keys: ['value_dec.zero_logit.0.weight', 'value_dec.zero_logit.0.bias', 'value_dec.zero_logit.2.weight', 'value_dec.zero_logit.2.bias', 'value_dec.zero_logit.4.weight', 'value_dec.zero_logit.4.bias', 'cellwise_dec.zero_logit.weight', 'cellwise_dec.zero_logit.bias']\n"
     ]
    }
   ],
   "source": [
    "net = Finetune_Cell_FM(cfg) # 27855\n",
    "net = Finetune_Cell_FM(cfg).to(cfg.device)\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = \"cls.\" in name or \"encoder\" in name\n",
    "\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "net = net.to(cfg.device)\n",
    "#net.extractor.load_model(weight=True, moment=False)\n",
    "\n",
    "net.extractor.load_torch_weight(\"/home/yinghsin/M2LAB/CellFM-torch/cellfm_80m_pretrained.pt\")\n",
    "\n",
    "optimizer = AdamW([p for p in net.parameters() if p.requires_grad], \n",
    "                    lr=1e-4,\n",
    "                    weight_decay=1e-5)\n",
    "\n",
    "scaler = GradScaler() \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinghsin/miniconda3/envs/scvi/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/yinghsin/miniconda3/envs/scvi/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json, time, os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm \n",
    "from scib_metrics import silhouette_label, silhouette_batch, bras\n",
    "import scib\n",
    "\n",
    "from TOSICA_scGPT.TOSICA_model import (\n",
    "    ClsDecoder,\n",
    "    Mlp_reconstruction,\n",
    "    ProjectionHead,\n",
    "    AdversarialHead,\n",
    "    DomainAffine,\n",
    "    DSBatchNorm,\n",
    ")\n",
    "from TOSICA_scGPT.train import mask_input, sclsc_margin_loss_batch\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CellFMWithMyHead(nn.Module):\n",
    "    \"\"\"\n",
    "    用 CellFM 的 cls_token 當 cell embedding (z)，\n",
    "    後面接你們原本 4 個 head + 4 個 loss。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cellfm_net,          # 你建立好的 net = Finetune_Cell_FM(cfg)\n",
    "        emb_dim: int,        # 1536\n",
    "        num_genes: int,\n",
    "        num_cell_types: int,\n",
    "        num_batches: int,\n",
    "        use_domain_affine: bool = False,\n",
    "        recons_hidden_dims=(256, 512),\n",
    "        contras_dim: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cellfm = cellfm_net             # 這個裡面有 extractor (Cell_FM)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_genes = num_genes\n",
    "        self.num_cell_types = num_cell_types\n",
    "        self.num_batches = num_batches\n",
    "        self.use_domain_affine = use_domain_affine\n",
    "\n",
    "        # 你們原本四個 head（你說你都搬好了，我就直接用同名）\n",
    "        if use_domain_affine:\n",
    "            self.domain_affine = DomainAffine(emb_dim, n_domain=num_batches)\n",
    "        else:\n",
    "            self.domain_affine = nn.Identity()\n",
    "\n",
    "        self.cls_head = ClsDecoder(in_features=emb_dim, n_cls=num_cell_types, nlayers=3)\n",
    "\n",
    "        self.dsbn = DSBatchNorm(emb_dim, n_domain=num_batches)\n",
    "        self.recon_head = Mlp_reconstruction(\n",
    "            in_features=emb_dim,\n",
    "            n_domain=num_batches,\n",
    "            hidden_dims=list(recons_hidden_dims),\n",
    "            out_features=num_genes,\n",
    "            act_layer=nn.ReLU,\n",
    "            drop=0.2,\n",
    "        )\n",
    "\n",
    "        self.contras_head = ProjectionHead(\n",
    "            input_dim=emb_dim,\n",
    "            hidden_dim1=512,\n",
    "            hidden_dim2=128,\n",
    "            output_dim=contras_dim,\n",
    "        )\n",
    "\n",
    "        self.adv_head = AdversarialHead(in_dim=emb_dim, n_batch=num_batches)\n",
    "\n",
    "    def encode(self, batch):\n",
    "        \"\"\"\n",
    "        batch 是 CellFM dataloader 吐出來的那包 dict：\n",
    "        raw_nzdata, dw_nzdata, ST_feat, nonz_gene, mask_gene, zero_idx, feat, batch_id...\n",
    "        \"\"\"\n",
    "        # CellFM tutorial 的 net(...) 回傳 cls_token\n",
    "        cls_logits, mask_loss, cls_token = self.cellfm(\n",
    "            raw_nzdata=batch[\"raw_nzdata\"],\n",
    "            dw_nzdata=batch[\"dw_nzdata\"],\n",
    "            ST_feat=batch[\"ST_feat\"],\n",
    "            nonz_gene=batch[\"nonz_gene\"],\n",
    "            mask_gene=batch[\"mask_gene\"],\n",
    "            zero_idx=batch[\"zero_idx\"],\n",
    "        )\n",
    "\n",
    "        z = cls_token  # [B, 1536]\n",
    "        # 如果你們要做 domain affine，就用 batch_id\n",
    "        if self.use_domain_affine:\n",
    "            z = self.domain_affine(z, batch[\"batch_id\"])\n",
    "\n",
    "        return z, mask_loss, cls_logits\n",
    "\n",
    "    def forward(self, batch, mode=\"cls\", λ=1.0):\n",
    "        z, mask_loss, _ = self.encode(batch)\n",
    "\n",
    "        if mode == \"cls\":\n",
    "            logits = self.cls_head(z)\n",
    "            return z, logits, mask_loss\n",
    "\n",
    "        if mode == \"recons\":\n",
    "            recons = self.recon_head(z, batch[\"batch_id\"])\n",
    "            return z, recons, mask_loss\n",
    "\n",
    "        if mode == \"contras\":\n",
    "            proj = self.contras_head(z)\n",
    "            return z, proj, mask_loss\n",
    "\n",
    "        if mode == \"adv\":\n",
    "            batch_logits = self.adv_head(z, λ=λ)\n",
    "            return z, batch_logits, mask_loss\n",
    "\n",
    "        raise ValueError(f\"Unknown mode={mode}\")\n",
    "\n",
    "    def forward_adv(self, z: torch.Tensor, λ: float = 1.0):\n",
    "        \"\"\"\n",
    "        跟原本 TOSICA 一樣的介面： core(model).forward_adv(latent, λ)\n",
    "        \"\"\"\n",
    "        return self.adv_head(z, λ=λ)\n",
    "\n",
    "    # ===================== loss functions =====================\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_classification(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_reconstruction(\n",
    "        recons: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # target: 原始 expression（log1p / scale 後都可以，但要一致）\n",
    "        if mask is None:\n",
    "            return F.mse_loss(recons, target)\n",
    "        else:\n",
    "            return F.mse_loss(recons[mask], target[mask])\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_adversarial(\n",
    "        batch_logits: torch.Tensor,\n",
    "        batch_labels: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return F.cross_entropy(batch_logits, batch_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_contrastive(\n",
    "        z: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "        epoch: int,\n",
    "        margin: float = 1.0,\n",
    "        alpha1: float = 1.0,\n",
    "        alpha2: float = 1.0,\n",
    "        alpha3: float = 1.0,\n",
    "        alpha4: float = 1.0,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        用你 train.py 裡的 sclsc_margin_loss_batch\n",
    "        \"\"\"\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return sclsc_margin_loss_batch(\n",
    "            z,\n",
    "            labels,\n",
    "            batch,\n",
    "            epoch,\n",
    "            margin=margin,\n",
    "            alpha1=alpha1,\n",
    "            alpha2=alpha2,\n",
    "            alpha3=alpha3,\n",
    "            alpha4=alpha4,\n",
    "        )\n",
    "\n",
    "    # ===================== 一次算四種 loss cellfm version==============\n",
    "    '''\n",
    "    def compute_total_loss_from_batch(\n",
    "        self,\n",
    "        batch: dict,\n",
    "        epoch: int,\n",
    "        λ_adv: float = 1.0,\n",
    "        w_cls: float = 1.0,\n",
    "        w_contras: float = 0.3,\n",
    "        w_recons: float = 0.3,\n",
    "        w_adv: float = 0.2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # labels\n",
    "        labels = batch[\"feat\"].long()\n",
    "        batch_labels = batch[\"batch_id\"].long()\n",
    "\n",
    "        # 1) cls forward\n",
    "        z, cls_logits, mask_loss = self.forward(batch, mode=\"cls\")\n",
    "        cls_loss = self.loss_classification(cls_logits, labels)\n",
    "\n",
    "        # 2) reconstruction：先用 CellFM 自己的 mask_loss（最穩）\n",
    "        recons_loss = mask_loss\n",
    "\n",
    "        # 3) adversarial\n",
    "        batch_logits = self.forward_adv(z, λ=λ_adv)\n",
    "        batch_loss = self.loss_adversarial(batch_logits, batch_labels)\n",
    "\n",
    "        # 4) contrastive（如果你的 forward(mode=\"contras\") 會回 proj）\n",
    "        _, proj, _ = self.forward(batch, mode=\"contras\")\n",
    "        contras_loss = self.loss_contrastive(\n",
    "            proj, labels, batch_labels, epoch, **kwargs\n",
    "        )\n",
    "\n",
    "        total = w_cls*cls_loss + w_recons*recons_loss + w_adv*batch_loss + w_contras*contras_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total,\n",
    "            \"cls_loss\": cls_loss,\n",
    "            \"recons_loss\": recons_loss,\n",
    "            \"batch_loss\": batch_loss,\n",
    "            \"contras_loss\": contras_loss,\n",
    "        }\n",
    "    '''\n",
    "    def compute_total_loss_from_batch(\n",
    "        self,\n",
    "        batch: dict,\n",
    "        epoch: int,\n",
    "        λ_adv: float = 1.0,\n",
    "        w_cls: float = 1.0,\n",
    "        w_contras: float = 0.3,\n",
    "        w_recons: float = 0.3,\n",
    "        w_adv: float = 0.2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # labels\n",
    "        labels = batch[\"feat\"].long()\n",
    "        batch_labels = batch[\"batch_id\"].long()\n",
    "\n",
    "        # ✅ backbone 只跑一次\n",
    "        z, mask_loss, _ = self.encode(batch)   # ← 你 encode 回傳是 (z, mask_loss, extra)\n",
    "\n",
    "        # 1) cls\n",
    "        cls_logits = self.cls_head(z)\n",
    "        cls_loss = self.loss_classification(cls_logits, labels)\n",
    "\n",
    "        # 2) reconstruction：先用 CellFM 自己的 mask_loss（最穩）\n",
    "        recons_loss = mask_loss\n",
    "\n",
    "        # 3) adversarial\n",
    "        batch_logits = self.adv_head(z, λ=λ_adv)   # 或 self.forward_adv(z, λ_adv)\n",
    "        batch_loss = self.loss_adversarial(batch_logits, batch_labels)\n",
    "\n",
    "        # 4) contrastive：✅ 不要再 forward，直接用 z\n",
    "        proj = self.contras_head(z)\n",
    "        contras_loss = self.loss_contrastive(\n",
    "            proj, labels, batch_labels, epoch, **kwargs\n",
    "        )\n",
    "\n",
    "        total = w_cls*cls_loss + w_recons*recons_loss + w_adv*batch_loss + w_contras*contras_loss\n",
    "        return {\n",
    "            \"loss\": total,\n",
    "            \"cls_loss\": cls_loss,\n",
    "            \"recons_loss\": recons_loss,\n",
    "            \"batch_loss\": batch_loss,\n",
    "            \"contras_loss\": contras_loss,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dataset', 'location', 'nGene', 'nUMI', 'patientGroup', 'percent.mito',\n",
      "       'protocol', 'sanger_type', 'size_factors', 'sampling_method', 'batch',\n",
      "       'cell_type', 'donor', 'n_genes', 'train'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# see lung.h5ad有何column, to set batch_id for meaningful adversarial loss\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\")\n",
    "print(adata.obs.columns)\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# cell type（你已經在做）\n",
    "adata.obs[\"str_batch\"] = adata.obs[\"batch\"].astype(str)\n",
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"].astype(\"category\").cat.codes\n",
    "adata.var[\"gene_name\"] = adata.var_names.tolist()\n",
    "\n",
    "adata.obs[\"str_cell_type\"] = adata.obs[\"cell_type\"].astype(str).tolist()\n",
    "adata.obs[\"cell_type_id\"] = adata.obs[\"str_cell_type\"].astype(\"category\").cat.codes\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# cell type（你原本就在做）\n",
    "adata.obs[\"celltype\"] = adata.obs[\"cell_type\"]\n",
    "\n",
    "# batch id（新增這個）\n",
    "adata.obs[\"batch_id\"] = (\n",
    "    adata.obs[\"batch\"]\n",
    "    .astype(str)\n",
    "    .astype(\"category\")\n",
    "    .cat.codes\n",
    ")\n",
    "\n",
    "# 設定給 config\n",
    "cfg.num_batches = adata.obs[\"batch_id\"].nunique()\n",
    "\n",
    "\n",
    "# ✅ batch 分群（重點）\n",
    "# adata.obs[\"batch_id\"] = adata.obs[\"sample_id\"].astype(\"category\").cat.codes\n",
    "# for col in adata.obs.columns:\n",
    "#     print(col, adata.obs[col].nunique())\n",
    "# batch_id = batch['batch_id'].to(cfg.device)\n",
    "batch = next(iter(train_loader))\n",
    "# print(batch[\"batch_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 16\n",
      "cfg.num_cls = 17\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# cell type（你原本就在做）\n",
    "adata.obs[\"celltype\"] = adata.obs[\"cell_type\"]\n",
    "\n",
    "# batch id（新增這個）\n",
    "adata.obs[\"batch_id\"] = (\n",
    "    adata.obs[\"batch\"]\n",
    "    .astype(str)\n",
    "    .astype(\"category\")\n",
    "    .cat.codes\n",
    ")\n",
    "\n",
    "# 設定給 config\n",
    "cfg.num_batches = adata.obs[\"batch_id\"].nunique()\n",
    "num_batches = int(adata.obs[\"batch_id\"].nunique())\n",
    "print(\"num_batches =\", num_batches)\n",
    "cfg.num_cls = int(batch[\"feat\"].max().item() + 1)\n",
    "print(\"cfg.num_cls =\", cfg.num_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1536]) torch.Size([2, 17]) tensor(67.2037, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 假設你已經有 train_loader\n",
    "cfg.device = \"cuda:0\"\n",
    "batch = next(iter(train_loader))\n",
    "for k in batch:\n",
    "    batch[k] = batch[k].to(cfg.device) if torch.is_tensor(batch[k]) else batch[k]\n",
    "\n",
    "model = CellFMWithMyHead(\n",
    "    cellfm_net=net,\n",
    "    emb_dim=cfg.enc_dims,           # 1536\n",
    "    num_genes=200,       # 先隨便填，下一步我再幫你對齊\n",
    "    num_cell_types=17,\n",
    "    num_batches=16,                  # 你現在好像 batch_id 都設 0，先填 1\n",
    ").to(cfg.device)\n",
    "\n",
    "z, logits, mask_loss = model(batch, mode=\"cls\")\n",
    "print(z.shape, logits.shape, mask_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonz_gene max = 25688\n",
      "nonz_gene min = 0\n",
      "n_genes (model) = 27855\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"nonz_gene max =\", batch[\"nonz_gene\"].max().item())\n",
    "print(\"nonz_gene min =\", batch[\"nonz_gene\"].min().item())\n",
    "print(\"n_genes (model) =\", net.extractor.net.n_genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14., 16.])\n",
      "torch.float32\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[\"feat\"])\n",
    "print(batch[\"feat\"].dtype)\n",
    "print(model.num_cell_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gene_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scvi/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'gene_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cellfm_gene_list = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/yinghsin/M2LAB/CellFM-torch/csv/expand_gene_info.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgene_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      2\u001b[39m cellfm_genemap = {g: i+\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cellfm_gene_list)}\n\u001b[32m      3\u001b[39m gene_id = cellfm_genemap.get(gene_name, \u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# 0 = unknown / pad\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scvi/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scvi/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'gene_name'"
     ]
    }
   ],
   "source": [
    "cellfm_gene_list = pd.read_csv(\"/home/yinghsin/M2LAB/CellFM-torch/csv/expand_gene_info.csv\")[\"gene_name\"]\n",
    "cellfm_genemap = {g: i+1 for i, g in enumerate(cellfm_gene_list)}\n",
    "gene_id = cellfm_genemap.get(gene_name, 0)  # 0 = unknown / pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_nzdata torch.float32 torch.Size([2, 2048]) 0.0 6.4710493087768555\n",
      "dw_nzdata torch.float32 torch.Size([2, 2048, 2]) 0.0 6.4710493087768555\n",
      "ST_feat torch.float32 torch.Size([2, 2]) 0.0305292047560215 0.14841999113559723\n",
      "nonz_gene torch.int32 torch.Size([2, 2048]) 0 25688\n",
      "mask_gene torch.float32 torch.Size([2, 2048]) 0.0 1.0\n",
      "zero_idx torch.float32 torch.Size([2, 2048]) 0.0 1.0\n",
      "celltype_label torch.int64 torch.Size([2]) 14 16\n",
      "batch_id torch.int64 torch.Size([2]) 0 0\n",
      "feat torch.float32 torch.Size([2]) 14.0 16.0\n",
      "num_cls = 10\n",
      "cfg.num_cls = 10\n",
      "cfg.num_batches = 16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcfg.num_cls =\u001b[39m\u001b[33m\"\u001b[39m, cfg.num_cls)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcfg.num_batches =\u001b[39m\u001b[33m\"\u001b[39m, cfg.num_batches)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbatch_labels min/max =\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mbatch_labels\u001b[49m.min().item(), batch_labels.max().item()) \u001b[38;5;66;03m#have error in this line\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlabel min/max =\u001b[39m\u001b[33m\"\u001b[39m, labels.min().item(), labels.max().item()) \u001b[38;5;66;03m#have error in this line\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'batch_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(\n",
    "            k,\n",
    "            v.dtype,\n",
    "            v.shape,\n",
    "            v.min().item(),\n",
    "            v.max().item()\n",
    "        )\n",
    "\n",
    "print(\"num_cls =\", cfg.num_cls)\n",
    "print(\"cfg.num_cls =\", cfg.num_cls)\n",
    "print(\"cfg.num_batches =\", cfg.num_batches)\n",
    "\n",
    "print(\"batch_labels min/max =\", batch_labels.min().item(), batch_labels.max().item()) #have error in this line\n",
    "print(\"label min/max =\", labels.min().item(), labels.max().item()) #have error in this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['raw_nzdata', 'dw_nzdata', 'ST_feat', 'nonz_gene', 'mask_gene', 'zero_idx', 'celltype_label', 'batch_id', 'feat'])\n",
      "feat dtype/shape/min/max: torch.float32 torch.Size([2]) 2.0 16.0\n",
      "batch_id dtype/shape/min/max: torch.int64 torch.Size([2]) 0 0\n",
      "cfg.num_cls = 10\n",
      "cfg.num_batches = 16\n",
      "check labels range ok?  False\n",
      "check batch range ok?   True\n"
     ]
    }
   ],
   "source": [
    "# 先拿一個 batch（CPU）\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# 只用 CPU 印\n",
    "print(\"keys:\", batch.keys())\n",
    "print(\"feat dtype/shape/min/max:\",\n",
    "      batch[\"feat\"].dtype, batch[\"feat\"].shape,\n",
    "      batch[\"feat\"].min().item(), batch[\"feat\"].max().item())\n",
    "\n",
    "print(\"batch_id dtype/shape/min/max:\",\n",
    "      batch[\"batch_id\"].dtype, batch[\"batch_id\"].shape,\n",
    "      batch[\"batch_id\"].min().item(), batch[\"batch_id\"].max().item())\n",
    "\n",
    "print(\"cfg.num_cls =\", cfg.num_cls)\n",
    "print(\"cfg.num_batches =\", cfg.num_batches)\n",
    "\n",
    "print(\"check labels range ok? \", batch[\"feat\"].max().item() < cfg.num_cls)\n",
    "print(\"check batch range ok?  \", batch[\"batch_id\"].max().item() < cfg.num_batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items():\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(v):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         batch[k] = \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m labels = batch[\u001b[33m\"\u001b[39m\u001b[33mfeat\u001b[39m\u001b[33m\"\u001b[39m].long()          \u001b[38;5;66;03m# cell type\u001b[39;00m\n\u001b[32m     19\u001b[39m batch_labels = batch[\u001b[33m\"\u001b[39m\u001b[33mbatch_id\u001b[39m\u001b[33m\"\u001b[39m].long() \u001b[38;5;66;03m# batch\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "cfg.device = \"cuda:0\"\n",
    "\n",
    "scaler = GradScaler()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "w_cls, w_recons, w_contras, w_adv = 1.0, 0.3, 0.3, 0.2\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(train_loader):\n",
    "    # --- 1) 把 batch 丟到 device ---\n",
    "    for k, v in batch.items():\n",
    "        if torch.is_tensor(v):\n",
    "            batch[k] = v.to(cfg.device)\n",
    "\n",
    "    #labels = batch[\"feat\"].long()          # cell type\n",
    "    labels = batch[\"feat\"].long().to(cfg.device)\n",
    "    batch_labels = batch[\"batch_id\"].long() # batch\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with autocast():\n",
    "        losses = model.compute_total_loss_from_batch(batch, epoch=10, λ_adv=1.0)\n",
    "        loss = losses[\"loss\"]\n",
    "\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        pred = logits.argmax(dim=1)\n",
    "        acc = (pred == labels).float().mean().item()\n",
    "        print(f\"step={step} loss={loss.item():.4f} cls={cls_loss.item():.4f} mask={mask_loss.item():.4f} acc={acc:.3f}\")\n",
    "        batch_logits = model.forward_adv(z, λ=1.0)\n",
    "        pred_batch = batch_logits.argmax(dim=1)\n",
    "        print(\"batch acc:\", (pred_batch == batch_labels).float().mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
