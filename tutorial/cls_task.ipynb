{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toturial 1: Fine-tuning CellFM for Cell Type Annotation\n",
    "In this tutorial, we will demonstrate how to fine-tune a pre-trained CellFM model to perform cell type annotation on a new single-cell dataset. The workflow consists of the following stages:\n",
    "\n",
    "1. Data Preprocessing\n",
    "\n",
    "2. Data Loading\n",
    "\n",
    "3. Model construct\n",
    "\n",
    "4. Weight Loading\n",
    "\n",
    "5. Fine-tuning\n",
    "\n",
    "6. Result Visualization\n",
    "\n",
    "Before starting, import the following packages and set up the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/yinghsin/M2LAB/CellFM-torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinghsin/miniconda3/envs/scvi/lib/python3.12/site-packages/scanpy/_utils/__init__.py:27: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('anndata')` instead.\n",
      "  from anndata import __version__ as anndata_version\n",
      "/home/yinghsin/miniconda3/envs/scvi/lib/python3.12/site-packages/scanpy/__init__.py:36: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('anndata')` instead.\n",
      "  if Version(anndata.__version__) >= Version(\"0.11.0rc2\"):\n",
      "/home/yinghsin/miniconda3/envs/scvi/lib/python3.12/site-packages/scanpy/readwrite.py:15: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('anndata')` instead.\n",
      "  if Version(anndata.__version__) >= Version(\"0.11.0rc2\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir(\"..\")\n",
    "os.chdir(\"/home/yinghsin/M2LAB/CellFM-torch\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from layers.utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from model import Cell_FM\n",
    "\n",
    "cfg = Config_80M()\n",
    "cfg.ecs_threshold = 0.8\n",
    "cfg.ecs = True\n",
    "cfg.add_zero = True\n",
    "cfg.pad_zero = True\n",
    "cfg.use_bs = 8 #this is batch_size batch size for training\n",
    "cfg.mask_ratio = 0.5\n",
    "### Main param ###\n",
    "cfg.dataset = \"Pancrm4\"\n",
    "cfg.feature_col = \"cell_type\"\n",
    "cfg.ckpt_path = \"/bigdat2/user/shanggny/checkpoint/para80m/6300w_18000_19479-1_38071.ckpt\"\n",
    "cfg.device = \"cuda:0\"\n",
    "cfg.epoch = 5\n",
    "#cfg.num_cls = 1 # by default 之後會改成cfg.num_cls = int(batch[\"feat\"].max().item() + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda device count:\", torch.cuda.device_count())\n",
    "cfg.device = \"cuda:0\"\n",
    "#cfg.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import pickle as pk\n",
    "import anndata as ad\n",
    "import requests as rq\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm,trange\n",
    "from functools import partial\n",
    "from scipy.sparse import csr_matrix as csr\n",
    "from scipy.sparse import csc_matrix as csc\n",
    "from multiprocessing import Process,Pool\n",
    "from sklearn.neighbors import NearestNeighbors as NN\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The data preprocessing workflow is identical to the original CellFM implementation. Please follow the same steps as described in the original [CellFM documentation](https://github.com/biomed-AI/CellFM/blob/main/tutorials/process.ipynb) to prepare your datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Load an .h5ad single-cell dataset and return a PyTorch DataLoader ready for model training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin shape: (32472, 15148)\n",
      "origin shape: (32472, 15148)\n"
     ]
    }
   ],
   "source": [
    "adata_path = \"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\"\n",
    "def load_data(adata_path, mode=\"train\"):\n",
    "    adata = read_h5ad(adata_path)\n",
    "    adata.obs['celltype'] = adata.obs['cell_type']\n",
    "\n",
    "    # 1) cell type label (固定全域 category)\n",
    "    cat = adata.obs[cfg.feature_col].astype(str).astype(\"category\")\n",
    "    adata.obs[\"feat\"] = cat.cat.codes.astype(np.int64)\n",
    "\n",
    "    # ✅ 用「總類別數」，不是 nunique()\n",
    "    cfg.num_cls = int(len(cat.cat.categories))\n",
    "\n",
    "    # adata.obs['batch_id'] = 0 #this is for single batch data, 會讓adv_loss變沒意義\n",
    "    # 1) cell type label\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(str)\n",
    "    adata.obs[\"feat\"] = adata.obs[cfg.feature_col].astype(\"category\").cat.codes.astype(np.int64)\n",
    "    cfg.num_cls = int(adata.obs[\"feat\"].nunique())\n",
    "    adata.obs[\"batch_id\"] = adata.obs[\"batch\"].astype(str).astype(\"category\").cat.codes.astype(np.int64)\n",
    "\n",
    "    cfg.num_batches = int(adata.obs[\"batch_id\"].nunique())\n",
    "\n",
    "    if mode == \"train\":\n",
    "        adata.obs['train'] = 0\n",
    "        dataset = SCrna(adata, mode=\"train\")\n",
    "        prep = Prepare(cfg.nonz_len, pad=0, mask_ratio=cfg.mask_ratio)\n",
    "        loader = build_dataset(\n",
    "            dataset,\n",
    "            prep=prep,\n",
    "            batch_size=cfg.use_bs,\n",
    "            pad_zero=cfg.pad_zero,\n",
    "            drop=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "    if mode== \"test\":\n",
    "        adata.obs['train'] = 2\n",
    "        dataset = SCrna(adata, mode=\"test\")\n",
    "        prep = Prepare(cfg.nonz_len, pad=0, mask_ratio=cfg.mask_ratio)\n",
    "        loader = build_dataset(\n",
    "            dataset,\n",
    "            prep=prep,\n",
    "            batch_size=cfg.use_bs,\n",
    "            drop=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "    return loader\n",
    "\n",
    "train_loader = load_data(\n",
    "    \"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\",\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "test_loader = load_data(\n",
    "    \"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\",\n",
    "    mode=\"test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin shape: (32472, 15148)\n",
      "labels min/max: 2 14\n",
      "cfg.num_cls: 17\n",
      "range ok: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# check labels range \n",
    "train_loader = load_data(\"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\", mode=\"train\")\n",
    "batch = next(iter(train_loader))\n",
    "labels = batch[\"feat\"].long()\n",
    "\n",
    "print(\"labels min/max:\", labels.min().item(), labels.max().item())\n",
    "print(\"cfg.num_cls:\", cfg.num_cls)\n",
    "print(\"range ok:\", (labels.min() >= 0) and (labels.max() < cfg.num_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd = /home/yinghsin/M2LAB/CellFM-torch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yinghsin/M2LAB/CellFM-torch\")\n",
    "print(\"cwd =\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construct\n",
    "To make CellFM more flexible and reusable across different tasks, we retain its core masked recovery module and re-implement it in PyTorch as a class called `Cell_FM`. This class can either:\n",
    "\n",
    "- Load a set of pre-trained weights and the corresponding configuration, or\n",
    "\n",
    "- Be trained from scratch.\n",
    "\n",
    "`Cell_FM` allows fine-tuning on new datasets to perform masked expression recovery and outputs a cls_token representing each cell.\n",
    "\n",
    "For tasks that require cell type classification, we can simply add a linear classification layer on top of the cls_token. In this tutorial, we use a single hidden-layer linear model for this purpose.\n",
    "\n",
    "Below, we demonstrate how to leverage the `Cell_FM` module to build a custom PyTorch model suitable for task-specific fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finetune_Cell_FM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Finetune_Cell_FM, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.num_cls = cfg.num_cls\n",
    "        self.extractor = Cell_FM(27855, self.cfg, ckpt_path=self.cfg.ckpt_path, device=self.cfg.device) # n_gene, cfg=config_80M()\n",
    "        # notion: 27855 is the orignal pre_train gene set of 80M CellFM model\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(self.cfg.enc_dims, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, self.num_cls)\n",
    "        )\n",
    "    \n",
    "    def forward(self, raw_nzdata,\n",
    "                dw_nzdata,\n",
    "                ST_feat,\n",
    "                nonz_gene,\n",
    "                mask_gene,\n",
    "                zero_idx):\n",
    "        \n",
    "        mask_loss, cls_token = self.extractor(\n",
    "                raw_nzdata,\n",
    "                dw_nzdata,\n",
    "                ST_feat,\n",
    "                nonz_gene,\n",
    "                mask_gene,\n",
    "                zero_idx\n",
    "            )\n",
    "        \n",
    "        cls = self.cls(cls_token)\n",
    "\n",
    "        return cls, mask_loss, cls_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Loading\n",
    "\n",
    "You can easily load pre-trained weights using the `load_model` function in the `Cell_FM` module. Setting the option to False will initialize the model from scratch.\n",
    "\n",
    "Note: During fine-tuning, we only unfreeze the `cls.` and `encoder` layers. This strategy reduces the computational cost and memory usage while still allowing effective adaptation to the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "extractor.net.encoder.0.attn.q_proj.weight\n",
      "extractor.net.encoder.0.attn.k_proj.weight\n",
      "extractor.net.encoder.0.attn.v_proj.weight\n",
      "extractor.net.encoder.0.attn.u_proj.weight\n",
      "extractor.net.encoder.0.attn.o_proj.weight\n",
      "extractor.net.encoder.0.ffn.u_proj.weight\n",
      "extractor.net.encoder.0.ffn.v_proj.weight\n",
      "extractor.net.encoder.0.ffn.o_proj.weight\n",
      "extractor.net.encoder.0.post_norm1.weight\n",
      "extractor.net.encoder.0.post_norm1.bias\n",
      "extractor.net.encoder.0.post_norm2.weight\n",
      "extractor.net.encoder.0.post_norm2.bias\n",
      "extractor.net.encoder.1.attn.q_proj.weight\n",
      "extractor.net.encoder.1.attn.k_proj.weight\n",
      "extractor.net.encoder.1.attn.v_proj.weight\n",
      "extractor.net.encoder.1.attn.u_proj.weight\n",
      "extractor.net.encoder.1.attn.o_proj.weight\n",
      "extractor.net.encoder.1.ffn.u_proj.weight\n",
      "extractor.net.encoder.1.ffn.v_proj.weight\n",
      "extractor.net.encoder.1.ffn.o_proj.weight\n",
      "extractor.net.encoder.1.post_norm1.weight\n",
      "extractor.net.encoder.1.post_norm1.bias\n",
      "extractor.net.encoder.1.post_norm2.weight\n",
      "extractor.net.encoder.1.post_norm2.bias\n",
      "cls.0.weight\n",
      "cls.0.bias\n",
      "cls.1.weight\n",
      "cls.1.bias\n",
      "cls.3.weight\n",
      "cls.3.bias\n",
      "[Load PyTorch Weight]\n",
      "Missing keys: []\n",
      "Unexpected keys: ['value_dec.zero_logit.0.weight', 'value_dec.zero_logit.0.bias', 'value_dec.zero_logit.2.weight', 'value_dec.zero_logit.2.bias', 'value_dec.zero_logit.4.weight', 'value_dec.zero_logit.4.bias', 'cellwise_dec.zero_logit.weight', 'cellwise_dec.zero_logit.bias']\n"
     ]
    }
   ],
   "source": [
    "net = Finetune_Cell_FM(cfg) # 27855\n",
    "net = Finetune_Cell_FM(cfg).to(cfg.device)\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = \"cls.\" in name or \"encoder\" in name\n",
    "\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "net = net.to(cfg.device)\n",
    "#net.extractor.load_model(weight=True, moment=False)\n",
    "\n",
    "net.extractor.load_torch_weight(\"/home/yinghsin/M2LAB/CellFM-torch/cellfm_80m_pretrained.pt\")\n",
    "\n",
    "optimizer = AdamW([p for p in net.parameters() if p.requires_grad], \n",
    "                    lr=1e-4,\n",
    "                    weight_decay=1e-5)\n",
    "\n",
    "scaler = GradScaler() \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json, time, os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm \n",
    "from scib_metrics import silhouette_label, silhouette_batch, bras\n",
    "import scib\n",
    "\n",
    "from TOSICA_scGPT.TOSICA_model import (\n",
    "    ClsDecoder,\n",
    "    Mlp_reconstruction,\n",
    "    ProjectionHead,\n",
    "    AdversarialHead,\n",
    "    DomainAffine,\n",
    "    DSBatchNorm,\n",
    ")\n",
    "from TOSICA_scGPT.train import mask_input, sclsc_margin_loss_batch\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CellFMWithMyHead(nn.Module):\n",
    "    \"\"\"\n",
    "    用 CellFM 的 cls_token 當 cell embedding (z)，\n",
    "    後面接你們原本 4 個 head + 4 個 loss。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cellfm_net,          # 你建立好的 net = Finetune_Cell_FM(cfg)\n",
    "        emb_dim: int,        # 1536\n",
    "        num_genes: int,\n",
    "        num_cell_types: int,\n",
    "        num_batches: int,\n",
    "        use_domain_affine: bool = False,\n",
    "        recons_hidden_dims=(256, 512),\n",
    "        contras_dim: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cellfm = cellfm_net             # 這個裡面有 extractor (Cell_FM)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_genes = num_genes\n",
    "        self.num_cell_types = num_cell_types\n",
    "        self.num_batches = num_batches\n",
    "        self.use_domain_affine = use_domain_affine\n",
    "\n",
    "        # 你們原本四個 head（你說你都搬好了，我就直接用同名）\n",
    "        if use_domain_affine:\n",
    "            self.domain_affine = DomainAffine(emb_dim, n_domain=num_batches)\n",
    "        else:\n",
    "            self.domain_affine = nn.Identity()\n",
    "\n",
    "        self.cls_head = ClsDecoder(in_features=emb_dim, n_cls=num_cell_types, nlayers=3)\n",
    "\n",
    "        self.dsbn = DSBatchNorm(emb_dim, n_domain=num_batches)\n",
    "        self.recon_head = Mlp_reconstruction(\n",
    "            in_features=emb_dim,\n",
    "            n_domain=num_batches,\n",
    "            hidden_dims=list(recons_hidden_dims),\n",
    "            out_features=num_genes,\n",
    "            act_layer=nn.ReLU,\n",
    "            drop=0.2,\n",
    "        )\n",
    "\n",
    "        self.contras_head = ProjectionHead(\n",
    "            input_dim=emb_dim,\n",
    "            hidden_dim1=512,\n",
    "            hidden_dim2=128,\n",
    "            output_dim=contras_dim,\n",
    "        )\n",
    "\n",
    "        self.adv_head = AdversarialHead(in_dim=emb_dim, n_batch=num_batches)\n",
    "\n",
    "    def encode(self, batch):\n",
    "        \"\"\"\n",
    "        batch 是 CellFM dataloader 吐出來的那包 dict：\n",
    "        raw_nzdata, dw_nzdata, ST_feat, nonz_gene, mask_gene, zero_idx, feat, batch_id...\n",
    "        \"\"\"\n",
    "        # CellFM tutorial 的 net(...) 回傳 cls_token\n",
    "        cls_logits, mask_loss, cls_token = self.cellfm(\n",
    "            raw_nzdata=batch[\"raw_nzdata\"],\n",
    "            dw_nzdata=batch[\"dw_nzdata\"],\n",
    "            ST_feat=batch[\"ST_feat\"],\n",
    "            nonz_gene=batch[\"nonz_gene\"],\n",
    "            mask_gene=batch[\"mask_gene\"],\n",
    "            zero_idx=batch[\"zero_idx\"],\n",
    "        )\n",
    "\n",
    "        z = cls_token  # [B, 1536]\n",
    "        # 如果你們要做 domain affine，就用 batch_id\n",
    "        if self.use_domain_affine:\n",
    "            z = self.domain_affine(z, batch[\"batch_id\"])\n",
    "\n",
    "        return z, mask_loss, cls_logits\n",
    "\n",
    "    def forward(self, batch, mode=\"cls\", λ=1.0):\n",
    "        z, mask_loss, _ = self.encode(batch)\n",
    "\n",
    "        if mode == \"cls\":\n",
    "            logits = self.cls_head(z)\n",
    "            return z, logits, mask_loss\n",
    "\n",
    "        if mode == \"recons\":\n",
    "            recons = self.recon_head(z, batch[\"batch_id\"])\n",
    "            return z, recons, mask_loss\n",
    "\n",
    "        if mode == \"contras\":\n",
    "            proj = self.contras_head(z)\n",
    "            return z, proj, mask_loss\n",
    "\n",
    "        if mode == \"adv\":\n",
    "            batch_logits = self.adv_head(z, λ=λ)\n",
    "            return z, batch_logits, mask_loss\n",
    "\n",
    "        raise ValueError(f\"Unknown mode={mode}\")\n",
    "\n",
    "    def forward_adv(self, z: torch.Tensor, λ: float = 1.0):\n",
    "        \"\"\"\n",
    "        跟原本 TOSICA 一樣的介面： core(model).forward_adv(latent, λ)\n",
    "        \"\"\"\n",
    "        return self.adv_head(z, λ=λ)\n",
    "\n",
    "    # ===================== loss functions =====================\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_classification(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_reconstruction(\n",
    "        recons: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # target: 原始 expression（log1p / scale 後都可以，但要一致）\n",
    "        if mask is None:\n",
    "            return F.mse_loss(recons, target)\n",
    "        else:\n",
    "            return F.mse_loss(recons[mask], target[mask])\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_adversarial(\n",
    "        batch_logits: torch.Tensor,\n",
    "        batch_labels: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return F.cross_entropy(batch_logits, batch_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_contrastive(\n",
    "        z: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "        epoch: int,\n",
    "        margin: float = 1.0,\n",
    "        alpha1: float = 1.0,\n",
    "        alpha2: float = 1.0,\n",
    "        alpha3: float = 1.0,\n",
    "        alpha4: float = 1.0,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        用你 train.py 裡的 sclsc_margin_loss_batch\n",
    "        \"\"\"\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return sclsc_margin_loss_batch(\n",
    "            z,\n",
    "            labels,\n",
    "            batch,\n",
    "            epoch,\n",
    "            margin=margin,\n",
    "            alpha1=alpha1,\n",
    "            alpha2=alpha2,\n",
    "            alpha3=alpha3,\n",
    "            alpha4=alpha4,\n",
    "        )\n",
    "\n",
    "    # ===================== 一次算四種 loss cellfm version==============\n",
    "    # this is the 0119 new version\n",
    "    def compute_total_loss_from_batch(\n",
    "        self,\n",
    "        batch: dict,\n",
    "        epoch: int,\n",
    "        λ_adv: float = 1.0,\n",
    "        w_cls: float = 1.0,\n",
    "        w_contras: float = 0.3,\n",
    "        w_recons: float = 0.3,\n",
    "        w_adv: float = 0.2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        labels = batch[\"feat\"].long()\n",
    "        batch_labels = batch[\"batch_id\"].long()\n",
    "\n",
    "        # ✅ 只 encode 一次\n",
    "        z, mask_loss, _ = self.encode(batch)   # z: [B,1536]\n",
    "\n",
    "        # 1) cls head\n",
    "        cls_logits = self.cls_head(z)\n",
    "        cls_loss = self.loss_classification(cls_logits, labels)\n",
    "\n",
    "        # 2) recons: 用 CellFM mask_loss（最穩）\n",
    "        recons_loss = mask_loss\n",
    "\n",
    "        # 3) adv head\n",
    "        batch_logits = self.adv_head(z, λ=λ_adv)\n",
    "        batch_loss = self.loss_adversarial(batch_logits, batch_labels)\n",
    "\n",
    "        # 4) contras head\n",
    "        proj = self.contras_head(z)\n",
    "        contras_loss = self.loss_contrastive(proj, labels, batch_labels, epoch, **kwargs)\n",
    "\n",
    "        total = w_cls*cls_loss + w_recons*recons_loss + w_adv*batch_loss + w_contras*contras_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total,\n",
    "            \"cls_loss\": cls_loss,\n",
    "            \"recons_loss\": recons_loss,\n",
    "            \"batch_loss\": batch_loss,\n",
    "            \"contras_loss\": contras_loss,\n",
    "            # 方便 debug / print\n",
    "            \"cls_logits\": cls_logits,\n",
    "            \"batch_logits\": batch_logits,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dataset', 'location', 'nGene', 'nUMI', 'patientGroup', 'percent.mito',\n",
      "       'protocol', 'sanger_type', 'size_factors', 'sampling_method', 'batch',\n",
      "       'cell_type', 'donor', 'n_genes', 'train'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# see lung.h5ad有何column, to set batch_id for meaningful adversarial loss\n",
    "# load data處做過，這邊可以不做或不跑\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"/home/yinghsin/M2LAB/CellFM-torch/lung_processed.h5ad\")\n",
    "print(adata.obs.columns)\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# cell type（你已經在做）\n",
    "adata.obs[\"str_batch\"] = adata.obs[\"batch\"].astype(str)\n",
    "adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"].astype(\"category\").cat.codes\n",
    "adata.var[\"gene_name\"] = adata.var_names.tolist()\n",
    "\n",
    "adata.obs[\"str_cell_type\"] = adata.obs[\"cell_type\"].astype(str).tolist()\n",
    "adata.obs[\"cell_type_id\"] = adata.obs[\"str_cell_type\"].astype(\"category\").cat.codes\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# cell type（你原本就在做）\n",
    "adata.obs[\"celltype\"] = adata.obs[\"cell_type\"]\n",
    "\n",
    "# batch id（新增這個）\n",
    "adata.obs[\"batch_id\"] = (\n",
    "    adata.obs[\"batch\"]\n",
    "    .astype(str)\n",
    "    .astype(\"category\")\n",
    "    .cat.codes\n",
    ")\n",
    "\n",
    "# 設定給 config\n",
    "cfg.num_batches = adata.obs[\"batch_id\"].nunique()\n",
    "\n",
    "\n",
    "# ✅ batch 分群（重點）\n",
    "# adata.obs[\"batch_id\"] = adata.obs[\"sample_id\"].astype(\"category\").cat.codes\n",
    "# for col in adata.obs.columns:\n",
    "#     print(col, adata.obs[col].nunique())\n",
    "# batch_id = batch['batch_id'].to(cfg.device)\n",
    "batch = next(iter(train_loader))\n",
    "# print(batch[\"batch_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id unique: tensor([ 1,  5,  9, 12, 14, 15])\n",
      "feat min/max: 0.0 14.0\n",
      "cfg.num_cls / num_batches: 17 16\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(\"batch_id unique:\", batch[\"batch_id\"].unique())\n",
    "print(\"feat min/max:\", batch[\"feat\"].min().item(), batch[\"feat\"].max().item())\n",
    "print(\"cfg.num_cls / num_batches:\", cfg.num_cls, cfg.num_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches = 16\n",
      "cfg.num_cls = 15\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "# cell type（你原本就在做）\n",
    "adata.obs[\"celltype\"] = adata.obs[\"cell_type\"]\n",
    "\n",
    "# batch id（新增這個）\n",
    "adata.obs[\"batch_id\"] = (\n",
    "    adata.obs[\"batch\"]\n",
    "    .astype(str)\n",
    "    .astype(\"category\")\n",
    "    .cat.codes\n",
    ")\n",
    "\n",
    "# 設定給 config\n",
    "cfg.num_batches = adata.obs[\"batch_id\"].nunique()\n",
    "num_batches = int(adata.obs[\"batch_id\"].nunique())\n",
    "print(\"num_batches =\", num_batches)\n",
    "cfg.num_cls = int(batch[\"feat\"].max().item() + 1)\n",
    "print(\"cfg.num_cls =\", cfg.num_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"feat\"] = (\n",
    "    adata.obs[cfg.feature_col].astype(\"category\").cat.codes.astype(np.int64)\n",
    ")\n",
    "labels = batch[\"feat\"].to(cfg.device).long()\n",
    "batch_labels = batch[\"batch_id\"].to(cfg.device).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CellFMWithMyHead.__init__() missing 4 required positional arguments: 'emb_dim', 'num_genes', 'num_cell_types', and 'num_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m batch[\u001b[33m\"\u001b[39m\u001b[33mfeat\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mfeat\u001b[39m\u001b[33m\"\u001b[39m].long()\n\u001b[32m      7\u001b[39m batch[\u001b[33m\"\u001b[39m\u001b[33mbatch_id\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mbatch_id\u001b[39m\u001b[33m\"\u001b[39m].long()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mCellFMWithMyHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m)\u001b[49m.to(cfg.device)\n\u001b[32m     10\u001b[39m z, logits, mask_loss = model(batch, mode=\u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(z.shape, logits.shape, mask_loss)\n",
      "\u001b[31mTypeError\u001b[39m: CellFMWithMyHead.__init__() missing 4 required positional arguments: 'emb_dim', 'num_genes', 'num_cell_types', and 'num_batches'"
     ]
    }
   ],
   "source": [
    "cfg.device = \"cpu\"\n",
    "batch = next(iter(train_loader))\n",
    "need = [\"raw_nzdata\",\"dw_nzdata\",\"ST_feat\",\"nonz_gene\",\"mask_gene\",\"zero_idx\",\"feat\",\"batch_id\"]\n",
    "for k in need:\n",
    "    batch[k] = batch[k].to(cfg.device)\n",
    "batch[\"feat\"] = batch[\"feat\"].long()\n",
    "batch[\"batch_id\"] = batch[\"batch_id\"].long()\n",
    "\n",
    "model = CellFMWithMyHead(...).to(cfg.device)\n",
    "z, logits, mask_loss = model(batch, mode=\"cls\")\n",
    "print(z.shape, logits.shape, mask_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels unique: tensor([ 0,  2,  3,  5,  9, 16])\n",
      "labels min/max: 0 16\n",
      "num_cls: 15\n",
      "range ok: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "labels = batch[\"feat\"].long()\n",
    "print(\"labels unique:\", torch.unique(labels))\n",
    "print(\"labels min/max:\", labels.min().item(), labels.max().item())\n",
    "print(\"num_cls:\", cfg.num_cls)\n",
    "print(\"range ok:\", (labels.min() >= 0) and (labels.max() < cfg.num_cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1536]) torch.Size([8, 17]) tensor(29.7994, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cfg.device = \"cuda:0\"\n",
    "batch = next(iter(train_loader))\n",
    "need = [\"raw_nzdata\",\"dw_nzdata\",\"ST_feat\",\"nonz_gene\",\"mask_gene\",\"zero_idx\",\"feat\",\"batch_id\"]\n",
    "for k in need:\n",
    "    batch[k] = batch[k].to(cfg.device)\n",
    "batch[\"feat\"] = batch[\"feat\"].long()\n",
    "batch[\"batch_id\"] = batch[\"batch_id\"].long()\n",
    "\n",
    "model = CellFMWithMyHead(\n",
    "    cellfm_net=net,\n",
    "    emb_dim=cfg.enc_dims,   # 1536\n",
    "    num_genes=200,          # 先隨便填，下一步我再幫你對齊\n",
    "    num_cell_types=17,\n",
    "    num_batches=16,                  # 你現在好像 batch_id 都設 0，先填 1\n",
    ").to(cfg.device)\n",
    "\n",
    "\n",
    "z, logits, mask_loss = model(batch, mode=\"cls\")\n",
    "print(z.shape, logits.shape, mask_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonz_gene max = 27781\n",
      "nonz_gene min = 0\n",
      "n_genes (model) = 27855\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"nonz_gene max =\", batch[\"nonz_gene\"].max().item())\n",
    "print(\"nonz_gene min =\", batch[\"nonz_gene\"].min().item())\n",
    "print(\"n_genes (model) =\", net.extractor.net.n_genes)\n",
    "\n",
    "print(batch[\"feat\"])\n",
    "print(batch[\"feat\"].dtype)\n",
    "print(model.num_cell_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_nzdata torch.float32 torch.Size([2, 2048]) 0.0 6.4710493087768555\n",
      "dw_nzdata torch.float32 torch.Size([2, 2048, 2]) 0.0 6.4710493087768555\n",
      "ST_feat torch.float32 torch.Size([2, 2]) 0.0305292047560215 0.14841999113559723\n",
      "nonz_gene torch.int32 torch.Size([2, 2048]) 0 25688\n",
      "mask_gene torch.float32 torch.Size([2, 2048]) 0.0 1.0\n",
      "zero_idx torch.float32 torch.Size([2, 2048]) 0.0 1.0\n",
      "celltype_label torch.int64 torch.Size([2]) 14 16\n",
      "batch_id torch.int64 torch.Size([2]) 0 0\n",
      "feat torch.float32 torch.Size([2]) 14.0 16.0\n",
      "num_cls = 10\n",
      "cfg.num_cls = 10\n",
      "cfg.num_batches = 16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcfg.num_cls =\u001b[39m\u001b[33m\"\u001b[39m, cfg.num_cls)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcfg.num_batches =\u001b[39m\u001b[33m\"\u001b[39m, cfg.num_batches)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbatch_labels min/max =\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mbatch_labels\u001b[49m.min().item(), batch_labels.max().item()) \u001b[38;5;66;03m#have error in this line\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlabel min/max =\u001b[39m\u001b[33m\"\u001b[39m, labels.min().item(), labels.max().item()) \u001b[38;5;66;03m#have error in this line\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'batch_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(\n",
    "            k,\n",
    "            v.dtype,\n",
    "            v.shape,\n",
    "            v.min().item(),\n",
    "            v.max().item()\n",
    "        )\n",
    "\n",
    "print(\"num_cls =\", cfg.num_cls)\n",
    "print(\"cfg.num_cls =\", cfg.num_cls)\n",
    "print(\"cfg.num_batches =\", cfg.num_batches)\n",
    "\n",
    "print(\"batch_labels min/max =\", batch_labels.min().item(), batch_labels.max().item()) #have error in this line\n",
    "print(\"label min/max =\", labels.min().item(), labels.max().item()) #have error in this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['raw_nzdata', 'dw_nzdata', 'ST_feat', 'nonz_gene', 'mask_gene', 'zero_idx', 'celltype_label', 'batch_id', 'feat'])\n",
      "feat dtype/shape/min/max: torch.float32 torch.Size([2]) 2.0 16.0\n",
      "batch_id dtype/shape/min/max: torch.int64 torch.Size([2]) 0 0\n",
      "cfg.num_cls = 10\n",
      "cfg.num_batches = 16\n",
      "check labels range ok?  False\n",
      "check batch range ok?   True\n"
     ]
    }
   ],
   "source": [
    "# 先拿一個 batch（CPU）\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# 只用 CPU 印\n",
    "print(\"keys:\", batch.keys())\n",
    "print(\"feat dtype/shape/min/max:\",\n",
    "      batch[\"feat\"].dtype, batch[\"feat\"].shape,\n",
    "      batch[\"feat\"].min().item(), batch[\"feat\"].max().item())\n",
    "\n",
    "print(\"batch_id dtype/shape/min/max:\",\n",
    "      batch[\"batch_id\"].dtype, batch[\"batch_id\"].shape,\n",
    "      batch[\"batch_id\"].min().item(), batch[\"batch_id\"].max().item())\n",
    "\n",
    "print(\"cfg.num_cls =\", cfg.num_cls)\n",
    "print(\"cfg.num_batches =\", cfg.num_batches)\n",
    "\n",
    "print(\"check labels range ok? \", batch[\"feat\"].max().item() < cfg.num_cls)\n",
    "print(\"check batch range ok?  \", batch[\"batch_id\"].max().item() < cfg.num_batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type at::Half without overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_total_loss_from_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mλ_adv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mw_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mw_recons\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw_recons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mw_contras\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw_contras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mw_adv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw_adv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     loss = out[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     35\u001b[39m scaler.scale(loss).backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 190\u001b[39m, in \u001b[36mCellFMWithMyHead.compute_total_loss_from_batch\u001b[39m\u001b[34m(self, batch, epoch, λ_adv, w_cls, w_contras, w_recons, w_adv, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# 4) contras head\u001b[39;00m\n\u001b[32m    189\u001b[39m proj = \u001b[38;5;28mself\u001b[39m.contras_head(z)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m contras_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_contrastive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m total = w_cls*cls_loss + w_recons*recons_loss + w_adv*batch_loss + w_contras*contras_loss\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    195\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: total,\n\u001b[32m    196\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcls_loss\u001b[39m\u001b[33m\"\u001b[39m: cls_loss,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_logits\u001b[39m\u001b[33m\"\u001b[39m: batch_logits,\n\u001b[32m    203\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mCellFMWithMyHead.loss_contrastive\u001b[39m\u001b[34m(z, labels, batch, epoch, margin, alpha1, alpha2, alpha3, alpha4)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m用你 train.py 裡的 sclsc_margin_loss_batch\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m z = F.normalize(z, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msclsc_margin_loss_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha1\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha2\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha3\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha4\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/M2LAB/CellFM-torch/TOSICA_scGPT/train.py:555\u001b[39m, in \u001b[36msclsc_margin_loss_batch\u001b[39m\u001b[34m(z, labels, batch, epoch, margin, alpha1, alpha2, alpha3, alpha4, q, lambda_cpt, detach_centroid, use_count_weight)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# neg_loss_same  = (hinge2 * neg_same_batch).sum()\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# neg_loss_cross = (hinge2 * neg_cross_batch).sum()\u001b[39;00m\n\u001b[32m    553\u001b[39m neg_loss = alpha3 * neg_loss_same + alpha4 * neg_loss_cross\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m KL_loss = \u001b[43mkl_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# ---------------- total ----------------\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# print(neg_loss.item(), cb_loss.item())\u001b[39;00m\n\u001b[32m    559\u001b[39m pair_loss = \u001b[32m1\u001b[39m * neg_loss + \u001b[32m1\u001b[39m * pos_loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/M2LAB/CellFM-torch/TOSICA_scGPT/train.py:323\u001b[39m, in \u001b[36mkl_loss\u001b[39m\u001b[34m(z, labels, batch, temperature, eps)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# mask self\u001b[39;00m\n\u001b[32m    322\u001b[39m self_mask = torch.eye(B, dtype=torch.bool, device=z.device)\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m sim = \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# softmax over all others (neighbors)\u001b[39;00m\n\u001b[32m    326\u001b[39m W = F.softmax(sim, dim=\u001b[32m1\u001b[39m)   \u001b[38;5;66;03m# [B,B]\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: value cannot be converted to type at::Half without overflow"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "cfg.device = \"cuda:0\"\n",
    "\n",
    "scaler = GradScaler()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "w_cls, w_recons, w_contras, w_adv = 1.0, 0.3, 0.3, 0.2\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(train_loader):\n",
    "    # 1) move to device\n",
    "    for k, v in batch.items():\n",
    "        if torch.is_tensor(v):\n",
    "            batch[k] = v.to(cfg.device)\n",
    "\n",
    "    batch[\"feat\"] = batch[\"feat\"].long()\n",
    "    batch[\"batch_id\"] = batch[\"batch_id\"].long()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with autocast():\n",
    "        out = model.compute_total_loss_from_batch(\n",
    "            batch,\n",
    "            epoch=10,\n",
    "            λ_adv=1.0,\n",
    "            w_cls=w_cls,\n",
    "            w_recons=w_recons,\n",
    "            w_contras=w_contras,\n",
    "            w_adv=w_adv,\n",
    "        )\n",
    "        loss = out[\"loss\"]\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        labels = batch[\"feat\"]\n",
    "        batch_labels = batch[\"batch_id\"]\n",
    "\n",
    "        pred = out[\"cls_logits\"].argmax(dim=1)\n",
    "        acc = (pred == labels).float().mean().item()\n",
    "\n",
    "        pred_batch = out[\"batch_logits\"].argmax(dim=1)\n",
    "        batch_acc = (pred_batch == batch_labels).float().mean().item()\n",
    "\n",
    "        print(\n",
    "            f\"step={step} loss={out['loss'].item():.4f} \"\n",
    "            f\"cls={out['cls_loss'].item():.4f} recons={out['recons_loss'].item():.4f} \"\n",
    "            f\"contras={out['contras_loss'].item():.4f} adv={out['batch_loss'].item():.4f} \"\n",
    "            f\"acc={acc:.3f} batch_acc={batch_acc:.3f}\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
